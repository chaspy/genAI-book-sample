#!/usr/bin/env python3
"""
2-1-1: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®è¨ˆæ¸¬
tiktoken ã‚’ä½¿ã£ã¦æ—¥æœ¬èªã¨è‹±èªã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚
"""

# pip install tiktoken
import tiktoken
import sys
from pathlib import Path

def count_tokens(text, encoding_name="cl100k_base"):
    """æŒ‡å®šã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—"""
    encoding = tiktoken.get_encoding(encoding_name)
    tokens = encoding.encode(text)
    return len(tokens)

def visualize_tokens(text, encoding_name="cl100k_base"):
    """ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²ã‚’å¯è¦–åŒ–"""
    encoding = tiktoken.get_encoding(encoding_name)
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³IDã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    token_ids = encoding.encode(text)
    
    # å„ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’æ–‡å­—åˆ—ã«ãƒ‡ã‚³ãƒ¼ãƒ‰
    tokens = []
    for token_id in token_ids:
        # ãƒˆãƒ¼ã‚¯ãƒ³IDã‹ã‚‰æ–‡å­—åˆ—ã«å¤‰æ›
        token_str = encoding.decode([token_id])
        tokens.append(token_str)
    
    return tokens, token_ids

def main():
    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—ï¼ˆGPT-4/3.5ç”¨ï¼‰
    encoding = tiktoken.get_encoding("cl100k_base")
    
    # å‡ºåŠ›ã‚’åé›†
    output_lines = []
    
    def print_and_collect(text=""):
        """æ¨™æº–å‡ºåŠ›ã«è¡¨ç¤ºã—ã€åŒæ™‚ã«å‡ºåŠ›ãƒªã‚¹ãƒˆã«è¿½åŠ """
        print(text)
        output_lines.append(text)
    
    print_and_collect("=" * 50)
    print_and_collect("ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®è¨ˆæ¸¬ã¨ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²ã®å¯è¦–åŒ–")
    print_and_collect("=" * 50)
    
    # åŸºæœ¬çš„ãªä¾‹
    japanese_text = "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ"
    english_text = "Hello World"
    
    print_and_collect(f"\nã€æ—¥æœ¬èªã€‘ '{japanese_text}'")
    jp_tokens, jp_ids = visualize_tokens(japanese_text)
    print_and_collect(f"ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(jp_tokens)} ãƒˆãƒ¼ã‚¯ãƒ³")
    print_and_collect(f"ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²: {' | '.join(f'[{token}]' for token in jp_tokens)}")
    print_and_collect(f"ãƒˆãƒ¼ã‚¯ãƒ³ID: {jp_ids}")
    
    print_and_collect(f"\nã€è‹±èªã€‘ '{english_text}'")
    en_tokens, en_ids = visualize_tokens(english_text)
    print_and_collect(f"ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(en_tokens)} ãƒˆãƒ¼ã‚¯ãƒ³")
    print_and_collect(f"ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²: {' | '.join(f'[{token}]' for token in en_tokens)}")
    print_and_collect(f"ãƒˆãƒ¼ã‚¯ãƒ³ID: {en_ids}")
    
    # ã‚ˆã‚Šè©³ç´°ãªæ¯”è¼ƒ
    print_and_collect("\n" + "=" * 50)
    print_and_collect("ãƒˆãƒ¼ã‚¯ãƒ³ã®è©³ç´°æ¯”è¼ƒ")
    print_and_collect("=" * 50)
    
    examples = [
        ("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "Database"),
        ("ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ", "Hello World"),
        ("äººå·¥çŸ¥èƒ½", "Artificial Intelligence"),
        ("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹", "Database index"),
    ]
    
    for jp_text, en_text in examples:
        jp_tokens_list, _ = visualize_tokens(jp_text)
        en_tokens_list, _ = visualize_tokens(en_text)
        jp_tokens = len(jp_tokens_list)
        en_tokens = len(en_tokens_list)
        ratio = jp_tokens / en_tokens if en_tokens > 0 else 0
        
        print_and_collect(f"\næ—¥æœ¬èª: {jp_text}")
        print_and_collect(f"  ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²: {' | '.join(f'[{t}]' for t in jp_tokens_list)}")
        print_and_collect(f"  â†’ {jp_tokens} ãƒˆãƒ¼ã‚¯ãƒ³")
        print_and_collect(f"è‹±èª: {en_text}")
        print_and_collect(f"  ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²: {' | '.join(f'[{t}]' for t in en_tokens_list)}")
        print_and_collect(f"  â†’ {en_tokens} ãƒˆãƒ¼ã‚¯ãƒ³")
        print_and_collect(f"æ¯”ç‡: {ratio:.2f}å€")
    
    # ã‚³ã‚¹ãƒˆè¨ˆç®—ã®ä¾‹
    print_and_collect("\n" + "=" * 50)
    print_and_collect("APIåˆ©ç”¨æ–™é‡‘ã®è¦‹ç©ã‚‚ã‚Šä¾‹")
    print_and_collect("=" * 50)
    
    # æƒ³å®š: 1æ—¥100å›ã®å•ã„åˆã‚ã›ã€å„1000ãƒˆãƒ¼ã‚¯ãƒ³
    daily_queries = 100
    tokens_per_query = 1000
    days_per_month = 30
    
    # GPT-4ã®æ–™é‡‘ï¼ˆä¾‹: $0.03/1000ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
    price_per_1k_tokens = 0.03
    
    monthly_tokens = daily_queries * tokens_per_query * days_per_month
    monthly_cost = (monthly_tokens / 1000) * price_per_1k_tokens
    
    print_and_collect(f"\nå‰ææ¡ä»¶:")
    print_and_collect(f"  - 1æ—¥ã®å•ã„åˆã‚ã›æ•°: {daily_queries}å›")
    print_and_collect(f"  - 1å›ã‚ãŸã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {tokens_per_query}ãƒˆãƒ¼ã‚¯ãƒ³")
    print_and_collect(f"  - æœˆé–“æ—¥æ•°: {days_per_month}æ—¥")
    print_and_collect(f"  - æ–™é‡‘: ${price_per_1k_tokens}/1000ãƒˆãƒ¼ã‚¯ãƒ³")
    
    print_and_collect(f"\nè¨ˆç®—çµæœ:")
    print_and_collect(f"  - æœˆé–“ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {monthly_tokens:,}ãƒˆãƒ¼ã‚¯ãƒ³")
    print_and_collect(f"  - æœˆé¡è²»ç”¨: ${monthly_cost:.2f} (ç´„{monthly_cost * 150:.0f}å††)")
    
    # é•·æ–‡ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆæ¸¬
    print_and_collect("\n" + "=" * 50)
    print_and_collect("é•·æ–‡ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²ã®å¯è¦–åŒ–")
    print_and_collect("=" * 50)
    
    long_text = """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ä»•çµ„ã¿ã§ã™ã€‚"""
    
    tokens_list, token_ids = visualize_tokens(long_text)
    tokens = len(tokens_list)
    
    print_and_collect(f"\nã‚µãƒ³ãƒ—ãƒ«æ–‡ç« ï¼ˆ{len(long_text)}æ–‡å­—ï¼‰:")
    print_and_collect(f"ã€Œ{long_text}ã€")
    print_and_collect(f"\nãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²:")
    # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ã‚„ã™ãè¡¨ç¤ºï¼ˆ10ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«æ”¹è¡Œï¼‰
    for i in range(0, len(tokens_list), 10):
        chunk = tokens_list[i:i+10]
        print_and_collect(f"  {' | '.join(f'[{t}]' for t in chunk)}")
    
    print_and_collect(f"\nãƒˆãƒ¼ã‚¯ãƒ³æ•°: {tokens}ãƒˆãƒ¼ã‚¯ãƒ³")
    print_and_collect(f"æ–‡å­—æ•°/ãƒˆãƒ¼ã‚¯ãƒ³æ•°æ¯”: {len(long_text) / tokens:.2f}")
    
    # ç‰¹æ®Šãªä¾‹ã®å¯è¦–åŒ–
    print_and_collect("\n" + "=" * 50)
    print_and_collect("ç‰¹æ®Šãªä¾‹ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²")
    print_and_collect("=" * 50)
    
    special_examples = [
        "AI",
        "äººå·¥çŸ¥èƒ½",
        "ChatGPT",
        "ğŸ˜Š",
        "123",
        "ã“ã‚“ã«ã¡ã¯ï¼",
        "Hello, World!",
        "B-tree",
        "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
        "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹",
    ]
    
    for text in special_examples:
        tokens_list, _ = visualize_tokens(text)
        print_and_collect(f"\n'{text}' â†’ {' | '.join(f'[{t}]' for t in tokens_list)} ({len(tokens_list)}ãƒˆãƒ¼ã‚¯ãƒ³)")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = Path("2-1-1-out.txt")
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(output_lines))
    
    print(f"\nâœ… å‡ºåŠ›ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")

if __name__ == "__main__":
    main()